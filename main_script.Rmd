---
title: "(C)-Los diez mejores-7"
author: "Jeorge Atherton & Paulina Castillo"
date: "`r Sys.Date()`"
output: pdf_document
---

---- DELETE ----
Git Reference 
    git init
    git add .
    git commit -m "Add existing project files to Git"
    git remote add origin https://github.com/cameronmcnz/example-website.git
    git push -u -f origin master


```{r, results="hide", message=FALSE, echo=F}
## load/install packages
packages = c("caret", "caretEnsemble", "car", "knitr","tidyverse", "GGally")
lapply(packages, require, character.only = TRUE)
options(scipen=99)
#options(digits=10)
theme_set(theme_bw())
```


# ---- Data Understanding ----


```{r, message=FALSE, echo=F}
## load data
raw_data_test = read_csv('Test/Test.csv',show_col_types = FALSE)
raw_data = read_csv('Train/Train.csv',show_col_types = FALSE)
str(raw_data, give.attr = FALSE)
```


Redefine datatypes:
```{r, echo=F}

chr_vars = c('sessionId', 'custId', 'keyword', 'referralPath', 'adContent',
             'adwordsClickInfo.gclId', 'networkDomain')

num_vars = c('visitNumber',
'timeSinceLastVisit',
'pageviews','revenue')

num_vars_test = c('visitNumber',
'timeSinceLastVisit',
'pageviews')

log_vars = c('isMobile',
'isTrueDirect', 'adwordsClickInfo.isVideoAd',
'bounces', 'newVisits','adwordsClickInfo.adNetworkType')

date_vars = c('date')

datetime_vars = c('visitStartTime')

factor_vars = setdiff(names(raw_data),
                      c(chr_vars,num_vars,log_vars,date_vars,datetime_vars))


factor_vars_test = setdiff(names(raw_data_test),
                           c(chr_vars,num_vars,log_vars,date_vars, datetime_vars))


raw_data_typed = raw_data %>%
  mutate_at(chr_vars, as.character) %>%
  mutate_at(num_vars, as.numeric) %>%
  mutate_at(log_vars, as.logical) %>%
  mutate_at(date_vars, as.Date) %>%
  mutate_at(datetime_vars, as_datetime) %>%
  mutate_at(factor_vars, as_factor) 

raw_data_typed_test = raw_data_test %>%
  mutate_at(chr_vars, as.character) %>%
  mutate_at(num_vars_test, as.numeric) %>%
  mutate_at(log_vars, as.logical) %>%
  mutate_at(date_vars, as.Date) %>%
  mutate_at(datetime_vars, as_datetime) %>%
  mutate_at(factor_vars_test, as_factor) 

```

Numeric Data Quality Report:
```{r, echo=F}
## helper functions
Q1<-function(x,na.rm=TRUE) {
quantile(x,na.rm=na.rm)[2]
}
Q3<-function(x,na.rm=TRUE) {
quantile(x,na.rm=na.rm)[4]
}

## -- Numeric Summary -- ##
myNumericSummary <- function(x){
c(length(x), 
  n_distinct(x), 
  sum(is.na(x)), 
  mean(x, na.rm=TRUE),
  min(x,na.rm=TRUE), 
  Q1(x,na.rm=TRUE),
  median(x,na.rm=TRUE),
  Q3(x,na.rm=TRUE),
  max(x,na.rm=TRUE), 
  sd(x,na.rm=TRUE))}

numericSummary = raw_data_typed %>%
  select(where(is.numeric)) %>%
  reframe(across(everything(), ~ myNumericSummary(.), .names = "{col}"))

numericSummary = cbind(
stat=c("n","unique","missing","mean","min","Q1","median","Q3","max","sd"),
numericSummary)


numericSummaryFinal = numericSummary %>%
pivot_longer(-stat, names_to = "variable", values_to = "value") %>%
pivot_wider(names_from = stat, values_from = value) %>%
mutate(missing_pct = 100*missing/n, unique_pct = 100*unique/n) %>%
select(variable, n, missing, missing_pct, unique, unique_pct, everything())

numericSummaryFinal
```
For the numeric variables, it looks like the there aren't many issues with 
missing values but there are definitely some outliers to look out for here. 


Factor Data Quality Report:
```{r, echo=F}
## helper functions
getmodes <- function(v, type = 1) {
  tbl <- table(v)
  m1 <- which.max(tbl)
  if (type == 1) {
    return(names(m1))  # 1st mode
  } else if (type == 2) {
    return(names(which.max(tbl[-m1])))  # 2nd mode
  } else if (type == -1) {
    return(names(which.min(tbl)))  # least common mode
  } else {
    stop("Invalid type selected")
  }
}

getmodesCnt <- function(v, type = 1) {
  tbl <- table(v)
  m1 <- which.max(tbl)
  if (type == 1) {
    return(max(tbl))  # 1st mode frequency
  } else if (type == 2) {
    return(max(tbl[-m1]))  # 2nd mode frequency
  } else if (type == -1) {
    return(min(tbl))  # least common frequency
  } else {
    stop("Invalid type selected")
  }
}

## issue is in this function here...
noNumericSummary <- function(x){
c(length(x), 
  n_distinct(x), 
  sum(is.na(x)), 
  getmodes(x, type=1), # 1st mode
  getmodesCnt(x,type=1), # 1st freq
  getmodes(x, type=2), # 2nd mode
  getmodesCnt(x,type=2), # 2nd freq
  getmodes(x, type=-1), # Less common
  getmodesCnt(x,type=-1) # Less common freq
  )
}

factorSummary = raw_data_typed %>%
  select(where(is.factor)) %>%
  reframe(across(everything(), ~ noNumericSummary(.), .names = "{col}"))

nonumericSummary = cbind(
stat=c("n","unique","missing","1st mode","1st mode freq","2nd mode",
       "2nd mode freq","least common","least common freq"),
factorSummary)

nonumericSummaryFinal <- nonumericSummary %>%
pivot_longer(-stat, names_to = "variable", values_to = "value") %>%
pivot_wider(names_from = stat, values_from = value) %>%
mutate(missing = as.numeric(missing),
         unique = as.numeric(unique),
         n = as.numeric(n)) %>%
mutate(missing_pct = 100*(missing/n),
unique_pct = 100*(unique/n)) %>%
select(variable, n, missing, missing_pct, unique, unique_pct, everything())

nonumericSummaryFinal
```
For factor data we can see a fair number of variables with large proportions 
of missing values. These will likely need to be dropped or handled as being 
another category in the data. It's possible that some of these sparse fields
do a good job of explaining revenue. 


###########################
Let's view some plots now:

```{r, echo=F}
raw_data_typed %>%
  select_if(is.numeric) %>%
  ggcorr() 
```
Definitely some extreme behavior here. Pageviews seems mildly correlated with
revenue but the others we can't tell at the transactional level.

How many customers actually bought anything?
```{r, echo=F}
raw_data_typed %>%
  group_by(custId) %>%
  summarize(bought = ifelse(sum(revenue)>0,1,0)) %>%
  ungroup() %>%
  summarize(mean(bought))
```
Looks like ~11% of customers actually ever bought anything...

Of those customers that bought anything, how much did they spend?
```{r, echo=F}
## of those customers that bought, what does that look like?
raw_data_typed %>%
  group_by(custId) %>%
  summarize(revenue = sum(revenue)) %>%
  filter(revenue>0)  %>%
  ggplot(aes(log(revenue))) +
  geom_histogram(bins=30) +
  labs(title="Natural Log Revenue for Customers that Bought")
```
So essentially the log-adjusted revenue is normal looking. There are now two big
facts: only about 1 in 10 customers actually ever buys anything, but those 
customers that do buy something follows a log adjusted normal distribution. Now
the question is which factors will predict a customer will buy anything at all?

Boxplots of 'bought' by continent:
```{r, echo=F}
raw_data_typed %>%
  group_by(continent,custId) %>%
  summarize(bought = ifelse(sum(revenue)>0,1,0)) %>%
  group_by(continent) %>%
  summarize(bought = mean(bought)) %>%
  ggplot(aes(x=continent, y=bought)) +
  geom_col() +
  scale_y_continuous(limits = c(0,0.50)) +
  labs(title="Proportion of Customers that Bought by Continent")
```
From this simple bar chart, we see clearly that people from the Americas are far 
more likely to buy something over the course of the year than from other 
continents. 


# ---- Data Preparation & Feature Engineering ----


Cast NAs in factors as extra level to preserve information & drop cases of
no page views. There are only 8 cases of no pageviews and they didn't 
buy anything.
```{r, echo=F}
data_clean = raw_data_typed %>%
  mutate_if(is.factor, fct_na_value_to_level, level="Other") %>%
  #filter(!is.na(pageviews)) %>%
  replace_na(list(bounces=0,newVisits=0,pageviews=1))

## test
data_clean_test = raw_data_typed_test %>%
  mutate_if(is.factor, fct_na_value_to_level, level="Other") %>%
  #filter(!is.na(pageviews)) %>%
  replace_na(list(bounces=0,newVisits=0, pageviews=1))
```

Add new derived features.
```{r, echo=F}
data_clean = data_clean %>%
  mutate(month = factor(month(date)),
         visit_hour = hour(visitStartTime))

data_clean_test = data_clean_test %>%
  mutate(month = factor(month(date)),
         visit_hour = hour(visitStartTime))
```


Group by customer ID & compute log transformation
```{r, echo=F}
getMode <- function(x) {
  unique_x <- unique(x)
  tabulated_x <- tabulate(match(x, unique_x))
  mode_value <- unique_x[which.max(tabulated_x)]
  return(mode_value) }
  
data_aggregated = data_clean %>%
    group_by(custId) %>%
    reframe(across(where(is.factor), getMode),
            across(where(is.logical), getMode),
            visitNumber = max(visitNumber),
            timeSinceLastVisit = mean(timeSinceLastVisit), 
            pageviews = sum(pageviews),
            revenue = sum(revenue),
            visit_hour = getMode(visit_hour)
            ) %>%
    mutate(log_revenue = log(revenue+1)) %>%
    select(-revenue)

## test 
data_aggregated_test = data_clean_test %>%
    group_by(custId) %>%
    reframe(across(where(is.factor), getMode),
            across(where(is.logical), getMode),
            visitNumber = max(visitNumber),
            timeSinceLastVisit = mean(timeSinceLastVisit), 
            pageviews = sum(pageviews),
            visit_hour = getMode(visit_hour)
            )

## call garbage collection after that massive aggregation
gc()
```


Collapse high cardinality factors. Remove redundant
geographic features. Remove useless columns.
```{r, echo=F}
fcts_drop = c("continent", "subContinent", "city", "metro", "region",
            "adwordsClickInfo.isVideoAd","adwordsClickInfo.adNetworkType",
            "adwordsClickInfo.page", "adwordsClickInfo.slot",
            "campaign", "topLevelDomain")

data_aggregated = data_aggregated %>% 
  select(!all_of(fcts_drop)) %>%
  mutate(browser=fct_lump_lowfreq(browser),
         operatingSystem=fct_lump_lowfreq(operatingSystem),
         source=fct_lump_lowfreq(source),
         country=fct_lump_n(country, n=5),
         channelGrouping=fct_lump_lowfreq(channelGrouping),
         deviceCategory=fct_lump_lowfreq(deviceCategory)) 
```
```{r}

## test

# saving the previous levels based on revenue from training because we dont have reveune in this data
browser_levels <- levels(data_aggregated$browser)
operating_system_levels <- levels(data_aggregated$operatingSystem)
source_levels <- levels(data_aggregated$source)
country_levels <- levels(data_aggregated$country)
channelGrouping_levels <- levels(data_aggregated$channelGrouping)
device_category_levels <- levels(data_aggregated$deviceCategory)

data_aggregated_test = data_aggregated_test %>% 
  select(!all_of(fcts_drop)) %>%
  mutate(browser = factor(browser, levels = browser_levels) %>% fct_na_value_to_level("Other"),
         operatingSystem = factor(operatingSystem, levels = operating_system_levels) %>% fct_na_value_to_level("Other"),
         source = factor(source, levels = source_levels) %>% fct_na_value_to_level("Other"),
         country = factor(country, levels = country_levels) %>% fct_na_value_to_level("Other"),
         channelGrouping = factor(channelGrouping, levels = channelGrouping_levels) %>% fct_na_value_to_level("Other"),
         deviceCategory = factor(deviceCategory, levels = device_category_levels) %>% fct_na_value_to_level("Other")
        )
```


# ---- Modeling ---- #


Parallel processing package to speed things up (DONT RUN IF LOW MEMORY)
```{r, echo=F}
library("doParallel")
cl <- makePSOCKcluster(8)
registerDoParallel(cl)
```

Create holdout set for model validation and define k-fold validation
```{r, echo=F}
trPos = createDataPartition(data_aggregated$log_revenue, p=0.75, list=F)
training = data_aggregated[trPos,]
testing = data_aggregated[-trPos,]
train_control= trainControl(method = "cv", number = 5)
```

Define list of regression models (NEED TO TUNE PARAMETERS)
```{r, echo=F}
models = caretList(
  log_revenue ~ . ,
  data = select(training, -custId),
  methodList = c("lm", "leapSeq","pls", "lasso", "earth"),
  trControl = train_control,
  metric = "RMSE")
print(summary(models))
gc()
```

check correlation of models
```{r, echo=F}
modelCor(resamples(models))
```

Ensemble model
```{r, echo=F}
ensemble = caretEnsemble(models)
print(summary(ensemble))
```


Make predictions on holdout testing data
```{r, echo=F}
model_test_pred = predict(ensemble, testing)
postResample(model_test_pred, testing$log_revenue)
```

Predict on actual testing set
```{r, echo=F}
predRevenue = predict(ensemble, data_aggregated_test)
FINAL_PREDICTIONS = as_tibble(list(custId = data_aggregated_test$custId,
                              predRevenue = pull(predRevenue)))
FINAL_PREDICTIONS %>% head()
```

Export to csv file
```{r, echo=F}
write.csv(FINAL_PREDICTIONS, 'predictions_file.csv', row.names = F)
```

Stop cluster
```{r, echo=F}
stopCluster(cl)
gc()
```



# ---- EXTRA CODE ---- 



Perform power transformations on numeric variables: (not set on keeping...)
```{r, echo=F}
pTransf = function(x) {
  p = powerTransform(x ~ 1, family="yjPower")
  return(yjPower(x,p$lambda))}

data_aggregated = data_aggregated %>%
  mutate(across(c("visitNumber","timeSinceLastVisit","pageviews"), pTransf))

data_aggregated_test = data_aggregated_test %>%
  mutate(across(c("visitNumber","timeSinceLastVisit","pageviews"), pTransf))

```



# ----- PAULINA SOME EXPERIMENTS --------




```{r}
raw_data <- raw_data %>%
  mutate(newVisits = replace_na(newVisits, 0))

```

```{r}
newVisits_summary <- raw_data %>%
  group_by(newVisits) %>%
  summarise(count = n())

# Print the summary of newVisits
print(newVisits_summary)
```



```{r}

# Calculating the 95% Confidence Interval for revenue
mean_revenue <- mean(raw_data$revenue, na.rm = TRUE)
std_error <- sd(raw_data$revenue, na.rm = TRUE) / sqrt(nrow(raw_data))

# 95% confidence interval calculation
ci_lower <- mean_revenue - qt(0.975, df = nrow(raw_data) - 1) * std_error
ci_upper <- mean_revenue + qt(0.975, df = nrow(raw_data) - 1) * std_error

# Display the confidence interval
cat("95% Confidence Interval for Revenue: [", ci_lower, ",", ci_upper, "]\n")
```

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)


# Filter data to exclude missing values in revenue and newVisits
data_new_visit <- raw_data %>% filter(!is.na(revenue), !is.na(newVisits))

# Separate data into two groups: first-time visitors and repeat visitors
first_time_visitors <- data_new_visit %>% filter(newVisits == 1)
repeat_visitors <- data_new_visit %>% filter(newVisits == 0)

```


```{r}
# Function to calculate 95% confidence interval for a given dataset
calc_confidence_interval <- function(data) {
  mean_revenue <- mean(data$revenue, na.rm = TRUE)
  std_error <- sd(data$revenue, na.rm = TRUE) / sqrt(nrow(data))
  
  ci_lower <- mean_revenue - qt(0.975, df = nrow(data) - 1) * std_error
  ci_upper <- mean_revenue + qt(0.975, df = nrow(data) - 1) * std_error
  
  return(c(ci_lower, ci_upper, mean_revenue))
}

# Calculate 95% confidence intervals for both groups
ci_first_time <- calc_confidence_interval(first_time_visitors)
ci_repeat <- calc_confidence_interval(repeat_visitors)

# Print confidence intervals
cat("95% Confidence Interval for First-Time Visitors Revenue: [", ci_first_time[1], ",", ci_first_time[2], "] with a mean of ", ci_first_time[3],"\n")
cat("95% Confidence Interval for Repeat Visitors Revenue: [", ci_repeat[1], ",", ci_repeat[2], "] with a mean of ",ci_repeat[3])

```

There's a difference between who's a new clients who's not 

```{r}

# Group by customer ID and calculate total visits per customer
visit_frequency <- raw_data %>%
  group_by(custId) %>%
  summarise(total_visits = n())

# Visualize the distribution of visit frequency
ggplot(visit_frequency, aes(x = total_visits)) +
  geom_histogram(binwidth = 1, fill = "blue", alpha = 0.7, color = "black") +
  labs(title = "Customer Visit Frequency", x = "Total Visits", y = "Number of Customers") +
  theme_minimal()

```
```{r}
# Group by customer ID and calculate total visits per customer
visit_frequency <- raw_data %>%
  group_by(custId) %>%
  summarise(total_visits = n())

# Calculate the percentage of the data for each total visit count
visit_percentage <- visit_frequency %>%
  group_by(total_visits) %>%
  summarise(count = n()) %>%
  mutate(percentage = (count / sum(count)) * 100) %>%
  arrange(desc(percentage))

# Get the top 5 most frequent visit counts by percentage
top_5_visits <- visit_percentage %>%
  top_n(5, wt = percentage)

# Print the top 5 most frequent visit counts with their percentages
print(top_5_visits)


```

```{r}
# Create a new column to indicate whether revenue was generated in the session
raw_data <- raw_data %>%
  mutate(revenue_status = ifelse(revenue > 0, "Purchase", "No Purchase"))

# Group by device category and revenue status and calculate the count
device_usage <- raw_data %>%
  group_by(deviceCategory, revenue_status) %>%
  summarise(count = n()) %>%
  ungroup()

# Plot device usage with a stacked bar plot showing total number of sessions
ggplot(device_usage, aes(x = deviceCategory, y = count, fill = revenue_status)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(title = "Device Usage by Customers (Purchase vs No Purchase)", 
       x = "Device Category", 
       y = "Number of Sessions") +
  scale_fill_manual(values = c("Purchase" = "black", "No Purchase" = "grey")) +
  theme_minimal()

# Add rate information
device_usage_rate <- raw_data %>%
  group_by(deviceCategory) %>%
  summarise(
    total_sessions = n(),
    purchase_sessions = sum(revenue > 0),
    no_purchase_sessions = sum(revenue == 0),
    purchase_rate = purchase_sessions / total_sessions * 100,
    no_purchase_rate = no_purchase_sessions / total_sessions * 100
  )

# Print the rate table
print(device_usage_rate)
```


```{r}
# Create a new column to indicate whether revenue was generated in the session
raw_data <- raw_data %>%
  mutate(revenue_status = ifelse(revenue > 0, "Purchase", "No Purchase"))

# Group by continent and revenue status and calculate the count
geo_distribution <- raw_data %>%
  group_by(continent, revenue_status) %>%
  summarise(count = n()) %>%
  ungroup()

# Plot geographic distribution with a stacked bar plot showing total number of sessions
ggplot(geo_distribution, aes(x = continent, y = count, fill = revenue_status)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(title = "Geographic Distribution of Customers (Purchase vs No Purchase)", 
       x = "Continent", 
       y = "Number of Sessions") +
  scale_fill_manual(values = c("Purchase" = "black", "No Purchase" = "grey")) +
  theme_minimal()

# Add rate information
geo_distribution_rate <- raw_data %>%
  group_by(continent) %>%
  summarise(
    total_sessions = n(),
    purchase_sessions = sum(revenue > 0),
    no_purchase_sessions = sum(revenue == 0),
    purchase_rate = purchase_sessions / total_sessions * 100,
    no_purchase_rate = no_purchase_sessions / total_sessions * 100
  )

# Print the rate table
print(geo_distribution_rate)
```
```{r}
# Analyze average pageviews per customer session
session_activity <- raw_data %>%
  group_by(custId) %>%
  summarise(avg_pageviews = mean(pageviews, na.rm = TRUE))

# Plot average pageviews
ggplot(session_activity, aes(x = avg_pageviews)) +
  geom_histogram(binwidth = 1, fill = "green", alpha = 0.7, color = "black") +
  labs(title = "Average Pageviews per Session", x = "Average Pageviews", y = "Number of Customers") +
  theme_minimal()

```







```{r}
# Convert visitStartTime to a readable date-time format (assuming visitStartTime is in UNIX timestamp format)
raw_data <- raw_data %>%
  mutate(visitStartTime = as_datetime(visitStartTime))

# Extract hour of visit start time and create revenue status column
raw_data <- raw_data %>%
  mutate(visit_hour = hour(visitStartTime),
         revenue_status = ifelse(revenue > 0, "Purchase", "No Purchase"))

# Group by continent, visit hour, and revenue status, and calculate the frequency
visit_start_by_continent <- raw_data %>%
  group_by(continent, visit_hour, revenue_status) %>%
  summarise(frequency = n()) %>%
  ungroup()

# Create separate plots for each continent
continents <- unique(visit_start_by_continent$continent)

# Loop through each continent and create individual plots with stacked bars for Purchase vs No Purchase
for (continent in continents) {
  continent_data <- visit_start_by_continent %>%
    filter(continent == !!continent)
  
  # Plot visit start time frequency for the specific continent with Purchase and No Purchase stacked
  plot <- ggplot(continent_data, aes(x = visit_hour, y = frequency, fill = revenue_status)) +
    geom_bar(stat = "identity", position = "stack") +
    labs(title = paste("Visit Start Time Frequency -", continent), 
         x = "Hour of the Day", 
         y = "Number of Sessions") +
    scale_fill_manual(values = c("Purchase" = "black", "No Purchase" = "grey")) +
    theme_minimal()
  
  # Print the plot for each continent
  print(plot)
}
```


